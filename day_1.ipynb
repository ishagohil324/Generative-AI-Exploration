{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Understanding LLMs - How ChatGPT Works\n",
        "\n",
        "large Language Models (LLMs) are deep learning models trained on massive amounts\n",
        "of text data to understand and generate human-like language.\n",
        "\n",
        "They learn:\n",
        "- Grammar\n",
        "- Context\n",
        "- Relationships between words\n",
        "- The ability to predict the next word in a sequence\n"
      ],
      "metadata": {
        "id": "7D8SG98ajpjR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSsQ1WZpjE9N",
        "outputId": "c73c66c7-bf84-40f2-e20a-78dafbe55337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Context: Generative AI helps us\n",
            " Predicted Next Word: innovate\n",
            " Final Sentence: Generative AI helps us innovate\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "context = \"Generative AI helps us\"\n",
        "possible_next_words = [\"create\", \"generate\", \"learn\", \"innovate\", \"understand\", \"dream\"]\n",
        "\n",
        "predicted = random.choice(possible_next_words)\n",
        "\n",
        "print(\" Context:\", context)\n",
        "print(\" Predicted Next Word:\", predicted)\n",
        "print(\" Final Sentence:\", context, predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown as md\n",
        "\n",
        "md(\"\"\"\n",
        "###  Transformers 101\n",
        "\n",
        "Transformers are the architecture behind most modern LLMs (like GPT, BERT, Claude).\n",
        "\n",
        "They work using:\n",
        "- **Attention Mechanism** → allows model to \"focus\" on important words in a sentence.\n",
        "- **Positional Encoding** → keeps track of word order.\n",
        "- **Decoder/Encoder blocks** → process and generate text.\n",
        "\n",
        "Example:\n",
        "Input: \"The cat sat on the\"\n",
        "Model predicts: \"mat\"\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "XItq4wQxm6ZO",
        "outputId": "27be3110-4f4d-41d7-d3fb-89a3a249c60b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n###  Transformers 101\n\nTransformers are the architecture behind most modern LLMs (like GPT, BERT, Claude).\n\nThey work using:\n- **Attention Mechanism** → allows model to \"focus\" on important words in a sentence.\n- **Positional Encoding** → keeps track of word order.\n- **Decoder/Encoder blocks** → process and generate text.\n\nExample:\nInput: \"The cat sat on the\"\nModel predicts: \"mat\"\n"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers --quiet\n",
        "\n",
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "\n",
        "prompt = \"UNIVERSE IS GO BIG THAT  \"\n",
        "result = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=60,   # generate more words\n",
        "    num_return_sequences=3,  # try 3 outputs\n",
        "    temperature=0.9,     # add creativity\n",
        "    top_p=0.95           # nucleus sampling\n",
        ")\n",
        "\n",
        "for i, r in enumerate(result):\n",
        "    print(f\"\\n--- Generated Text {i+1} ---\")\n",
        "    print(r[\"generated_text\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83AZROcAoPkK",
        "outputId": "27e1d387-39b7-4ff1-d718-c43e67b8ce74"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generated Text 1 ---\n",
            "UNIVERSE IS GO BIG THAT                   \n",
            "\n",
            "[Listed in the original post]\n",
            "\n",
            "--- Generated Text 2 ---\n",
            "UNIVERSE IS GO BIG THAT     *\n",
            "\n",
            "In the last few days of this week’s Christmas season, a new version of the world-famous Christmas tree was installed at our shop, at Wiena, but the tree was actually only built in one block from where we started. We were hoping that with\n",
            "\n",
            "--- Generated Text 3 ---\n",
            "UNIVERSE IS GO BIG THAT                       \n",
            "There was a big, big, big change in the US in 2001. At one point, the US decided to allow the transfer of government power to the next level, but in that time\n"
          ]
        }
      ]
    }
  ]
}